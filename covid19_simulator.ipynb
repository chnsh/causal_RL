{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning of Social Distancing during COVID-19 with a Confounder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Given a problem, the objective of reinforcement learning is to choose an optimal policy that maximizes expected reward over time. In other words, it takes for granted that we can learn a direct cause-effect relationship between following a particular policy and receiving a reward. However, an understanding of the principles of causality will tell you that this direct relationship is not always so easy to estimate. The aim of this project is to investigate the impact of a backdoor path through a causal confounder on reinforcement learning performance. We chose to use a relevant example of a social distancing policy during COVID-19 to illustrate this concept. We expect to find that an agent that learns a policy without adjusting for a confounder will perform disproportionately worse than adjusted policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an SEIHRD (Susceptible-Exposed-Infected-Hospitalized-Recovered or Dead) epidemic model to model the spread of COVID-19. An agent is assumed to be in one of the possible states at any given time-step, and transitions from one state to another with a given parameter governed by a set of differential equations.\n",
    "\n",
    "The different states and possible transitions are described in the diagram below:\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"figs/SEIRDH.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-deep')\n",
    "plt.style.use('seaborn-poster')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import whynot.gym as gym\n",
    "from whynot import utils\n",
    "%matplotlib inline\n",
    "import simulators.covid19 as covid19\n",
    "import simulators.covid19.environments.starter_env\n",
    "\n",
    "#from scripts import utils\n",
    "\n",
    "\n",
    "#import whynot.simulators.covid19 as covid19\n",
    "#import whynot.simulators.covid19.environments.starter_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter environment\n",
    "\n",
    "This environment helps us demonstrate our basic COVID-19 environment.\n",
    "\n",
    "### State space\n",
    "\n",
    "In line with our SEIRHD Model, the state space for our problem consists of:\n",
    "- Susceptible\n",
    "- Exposed\n",
    "- Infected\n",
    "- Recovered \n",
    "- Hospitalized\n",
    "- Dead\n",
    "\n",
    "### Transitions\n",
    "\n",
    "We can only transition from one state to another in the order dictated by the acronym SEIRHD. We cannot jump around from Dead to Infected, or Recovered to Exposed, for instance.\n",
    "\n",
    "### Action space\n",
    "\n",
    "We chose a discrete action space with values representing varying degrees of one action: social distancing.\n",
    "\n",
    "At a given timesteps, an agent can take any one of these actions:\n",
    "- 0% Social Distancing\n",
    "- 10% Social Distancing\n",
    "- 25% Social Distancing\n",
    "- 50% Social Distancing\n",
    "- 100% Social Distancing\n",
    "\n",
    "In the environment, social distancing translates to scaling the $\\text{beta}$ paramater: this represents the average number of contacts per person in a timestep.\n",
    "\n",
    "#### Reasoning: Single Action\n",
    "\n",
    "In order to clearly show the effect of a confounder on a reinforcement learning agent, we found it best to look at a simple action space, and understand how the policy for that action changes with different levels of a confounder. For this reason, the only action our agent can take is social distancing.\n",
    "\n",
    "Here are some other potential actions that we could implement in this environment: increasing hospital beds, increasing ventilators in a hospital.\n",
    "\n",
    "#### Reasoning: Discrete Action Space\n",
    "\n",
    "We found it logical to assume that a government intervention could be one of a discrete set of actions instead of using a distribution over a continuous action space.\n",
    "\n",
    "We did explore an implementation of actor-critic to model social distancing as a beta distribution that we sample from to compare the performance on a continuous space. However, incorporating this agent into our project proved to be a non-trivial task.\n",
    "\n",
    "### Reward function\n",
    "\n",
    "#### Starter reward: \n",
    "A simplistic reward function that does not consider cost of control measures\n",
    "\n",
    "$$\\text{reward} = \\text{value of individual} * (\\text{-state.deceased + state.susceptible})$$\n",
    "\n",
    "This is a reward function to demostrate the effect of policies on the curve with the sole aim of reducing the number of deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('COVID19-v0')\n",
    "env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_social_distancing_map = {\n",
    "        0: 1.0,\n",
    "        1: 0.75,\n",
    "        2: 0.5,\n",
    "        3: 0.25,\n",
    "        4: 0.10,\n",
    "        5: 0.0\n",
    "}\n",
    "\n",
    "social_distancing_to_action_map = {value:key for (key, value) in action_to_social_distancing_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_distancing_to_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "We have five simple policies to demonstrate our basic COVID-19 Environment.\n",
    "\n",
    "We will look at the effect of these policies on the different disease curves, to see how they flatten under each one:\n",
    "- No treatment\n",
    "- 10% Social Distancing\n",
    "- 25% Social Distancing\n",
    "- 50% Social Distancing\n",
    "- 100% Social Distancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTreatmentPolicy():\n",
    "    def sample_action(self, obs):\n",
    "        return 5\n",
    "    \n",
    "class SocialDistancingPolicy():\n",
    "    def __init__(self, social_distance_val):\n",
    "        self.social_distance_val = social_distance_val\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        return social_distancing_to_action_map[self.social_distance_val] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {\n",
    "    \"Social Distance 10%\": SocialDistancingPolicy(0.1),\n",
    "    \"Social Distance 25%\": SocialDistancingPolicy(0.25),\n",
    "    \"Social Distance 50%\": SocialDistancingPolicy(0.5),\n",
    "    \"Social Distance 100%\": SocialDistancingPolicy(1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_policies(default_policies, policy, policy_name):\n",
    "    policies = dict(default_policies)    \n",
    "    policies[policy_name] = policy\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_episode_length):\n",
    "    \"\"\"Sample a single trajectory, acting according to the specified policy.\"\"\"\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset()\n",
    "    obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the most recent observation to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = policy.sample_action(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        # Take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # Record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # End the rollout if the rollout ended\n",
    "        # Note that the rollout can end due to done, or due to max_episode_length\n",
    "        if done or steps > max_episode_length:\n",
    "            rollout_done = 1\n",
    "        else:\n",
    "            rollout_done = 0\n",
    "        terminals.append(rollout_done)\n",
    "        if rollout_done:\n",
    "            break\n",
    "\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_sample_trajectory(env, policies,title):\n",
    "    \"\"\"Plot sample trajectories from policies.\"\"\"\n",
    "    obs_dim_names = covid19.State.variable_names()\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, sharex=True, figsize=[30, 15])\n",
    "    axes = axes.flatten()    \n",
    "    plt.suptitle(title,size=32)\n",
    "    \n",
    "    for name, policy in policies.items():\n",
    "        trajectory = sample_trajectory(env, policy, 400)\n",
    "        obs = trajectory[\"observation\"]\n",
    "        # Plot state evolution\n",
    "        for i in range(len(obs_dim_names)):\n",
    "            y = obs[:, i]\n",
    "            axes[i].plot(y, label=name)\n",
    "            axes[i].set_ylabel(obs_dim_names[i])\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "            axes[i].set_ylim(np.minimum(ymin, y.min()), np.maximum(ymax, y.max()))\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "        \n",
    "        # Plot actions\n",
    "        actions = np.array(trajectory[\"action\"])\n",
    "        action_vals = [1 - action_to_social_distancing_map[action] for action in actions]\n",
    "\n",
    "        # actionlist = [actionlist_beta,actionlist_hosp,actionlist_rec]\n",
    "        for idx, label in enumerate([\"beta_scale\"]):\n",
    "            ax_idx = len(obs_dim_names) + idx\n",
    "            axes[ax_idx].plot(action_vals, label=name)\n",
    "            axes[ax_idx].set_ylabel(label)\n",
    "        \n",
    "        # Plot reward\n",
    "        reward = trajectory[\"reward\"]\n",
    "        axes[-1].plot(reward, label=name)\n",
    "        axes[-1].set_ylabel(\"reward\")\n",
    "        axes[-1].ticklabel_format(scilimits=(-2, 2))\n",
    "        ymin, ymax = axes[-1].get_ylim()\n",
    "        axes[-1].set_ylim(np.minimum(ymin, reward.min()), np.maximum(ymax, reward.max()))\n",
    "        \n",
    "    for ax in axes:\n",
    "        # ax.legend()\n",
    "        ax.set_xlabel(\"Day\")\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right')\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_trajectory(env, policies,title):\n",
    "    \"\"\"Plot sample trajectories from policies.\"\"\"\n",
    "    obs_dim_names = covid19.State.variable_names()\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, sharex=True, figsize=[30, 15])\n",
    "    axes = axes.flatten()    \n",
    "    plt.suptitle(title,size=32)\n",
    "    \n",
    "    for name, policy in policies.items():\n",
    "        trajectory = sample_trajectory(env, policy, 400)\n",
    "        obs = trajectory[\"observation\"]\n",
    "        # Plot state evolution\n",
    "        for i in range(len(obs_dim_names)):\n",
    "            y = obs[:, i]\n",
    "            axes[i].plot(y, label=name)\n",
    "            axes[i].set_ylabel(obs_dim_names[i])\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "            axes[i].set_ylim(np.minimum(ymin, y.min()), np.maximum(ymax, y.max()))\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "        \n",
    "        # Plot actions\n",
    "        actions = np.array(trajectory[\"action\"])\n",
    "        action_vals = [1 - action_to_social_distancing_map[action] for action in actions]\n",
    "\n",
    "        # actionlist = [actionlist_beta,actionlist_hosp,actionlist_rec]\n",
    "        for idx, label in enumerate([\"beta_scale\"]):\n",
    "            ax_idx = len(obs_dim_names) + idx\n",
    "            axes[ax_idx].plot(action_vals, label=name)\n",
    "            axes[ax_idx].set_ylabel(label)\n",
    "        \n",
    "        # Plot reward\n",
    "        reward = trajectory[\"reward\"]\n",
    "        axes[-1].plot(reward, label=name)\n",
    "        axes[-1].set_ylabel(\"reward\")\n",
    "        axes[-1].ticklabel_format(scilimits=(-2, 2))\n",
    "        ymin, ymax = axes[-1].get_ylim()\n",
    "        axes[-1].set_ylim(np.minimum(ymin, reward.min()), np.maximum(ymax, reward.max()))\n",
    "        \n",
    "    for ax in axes:\n",
    "        # ax.legend()\n",
    "        ax.set_xlabel(\"Day\")\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right')\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_bars(env, policies,title):\n",
    "    rewards = []\n",
    "\n",
    "    #sampling trajectory to collect rewards\n",
    "    for name, policy in policies.items():\n",
    "            trajectory = sample_trajectory(env, policy, 400)\n",
    "            rewards.append(sum(trajectory[\"reward\"]))\n",
    "            \n",
    "    #sorting policies by reward\n",
    "    plotlist = sorted(list(zip(list(policies.keys()),rewards)), key=lambda x: x[1], reverse=True)\n",
    "    plt.bar([x[0] for x in plotlist],[x[1] for x in plotlist])\n",
    "    plt.title(title, size=14)\n",
    "    plt.xticks(rotation=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_sample_trajectory(env, augment_policies(policies, NoTreatmentPolicy(), policy_name=\"No Treatment\"), \"COVID-19 Curves with Starter Reward Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(env, augment_policies(policies, NoTreatmentPolicy(), policy_name=\"No Treatment\"), \"Starter reward for different policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Policies on the Basic Environment\n",
    "\n",
    "As we can see, with no associated costs for implementing social distancing, the best option for us is to perform 100% Social Distancing - this ensures that nobody transitions beyond the susceptible stage, and has the highest reward.\n",
    "\n",
    "However, for more practical policies, we can observe that even a difference from 10% to 25% social distancing flattens the curve significantly more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of a Confounder\n",
    "\n",
    "<img style=\"float: center;\" src=\"figs/confounder.png\">\n",
    "\n",
    "We now wish to demonstrate the effect of an unobserved confounder on reinforcement learning. In the examples that follow we have **location** as an unobserved confounder. Our action space still remains the same: the strength of social distancing applied to the environment. However we now introduce two new variables to our environment:\n",
    "\n",
    "### 1. **Impact on Policy: Non-adherence factor** \n",
    "\n",
    "Non-adherence is the amount of social distancing policy violation in a given environment.\n",
    "\n",
    "### 2. **Impact on Reward: Economic cost of social distancing**\n",
    "\n",
    "This represents the lost economic output as a function of the amount of social distancing applied\n",
    "\n",
    "These added variables now allow us to implement a confounder that affects both the policy and the reward. To present a clearer picture, we take a simplistic example of these two fictional cities:\n",
    "\n",
    "### 1. **Tech Haven**\n",
    "\n",
    "A tech haven which has a high economic contribution per capita. Due to the nature of the work, furloughs and transitions to remote work are easier to implement, thereby ensuring workers adhere to social distancing policies without much effect on livelihood. \n",
    "This location would have a high adherence to social distancing policies, but also a high cost of social distancing.\n",
    "\n",
    "### 2. **Steel Town**\n",
    "\n",
    "A manufacturing city where economic contribution per capita is comparitively low. Steel plants play a central role in the economy - making remote work impossible, and forcing workers that rely on a daily wage to make ends meet to go into work, thereby violating social distancing policies.\n",
    "This location would have lower adherence to social distancing policies, but a low cost of social distancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Confounder\n",
    "\n",
    "### Confounded action space\n",
    "\n",
    "Our non-adherence is implemented as a function of the social distancing levied on a location. We can imagine that if no social distancing is applied, there is zero non-adherence. Accordingly, we define a non-adherence factor ($\\eta$) as follows:\n",
    "\n",
    "$$\\text{current social distancing} = \\text{current social distancing}(1 - \\eta)$$\n",
    "\n",
    "$\\eta = 0.1$ for **Steel Town** and $\\eta = 0.01$ for **Tech Haven**\n",
    "\n",
    "### Confounder Reward\n",
    "\n",
    "A reward function that weighs the costs and benefits of controlling COVID-19.\n",
    "\n",
    "$$\\text{reward} = \\text{value of individual} * (\\text{-state.deceased + state.susceptible}) - \\text{economic output per time} * \\text{current social distancing}$$\n",
    "\n",
    "This reward function is a simple linear function that aims to minimize deaths from the virus, as well as the cost of controlling its spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications of the Confounder\n",
    "\n",
    "We will explore the effect of a confounder by first artificially fixing its value (thereby indirectly adjusting for the given level of confounder), and then having it vary probabilistically. We expect to see that when we fix the level of the confounder, our agent will learn an optimal policy for the given level of the confounder. However, in real life, we do not have a way to adjust for a causal confounder in reinforcement learning. Therefore, the policy learned when we don't account for the confounder will do disproportionately worse than the adjusted policies on these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the Confounder Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at Tech Haven\n",
    "\n",
    "In this environment, there there is a cost to losing people to COVID, but there is a much higher cost associated with social distancing because of the high economic output of this place - so social distancing is very expensive in this location.\n",
    "\n",
    "Once we train our policy on this specific environment, we hope to see the policy learning to avoid stricter social distancing rules, while keeping the spread in check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.rich_place_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_env = gym.make('COVID19-RICH-v0')\n",
    "rich_env.seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Policy Gradient Agent\n",
    "\n",
    "### A simple overview of a Policy Gradient implementation:\n",
    "\n",
    "For a given state $s$, a policy can be written as a probability distribution $\\pi_\\theta(s, a)$ over actions $a$, where $\\theta$ represents the parameters the policy.\n",
    "\n",
    "The objective here is to learn a $\\theta^*$ that maximizes the objective function\n",
    "\n",
    "   $\\;\\;\\;\\; J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[r(\\tau)]$,\n",
    "\n",
    "where $\\tau$ is the trajectory sampled according to policy $\\pi_\\theta$ and $r(\\tau)$ is the discounted sum of rewards on $\\tau$.\n",
    "\n",
    "The policy gradient approach is to take the gradient of this objective\n",
    "\n",
    "$\\nabla_{\\theta} J (\\theta)\\\\\n",
    "= \\nabla_{\\theta} \\int\\pi_{\\theta}(\\tau)r(\\tau)d\\tau\\\\\n",
    " = \\int\\pi_{\\theta}(\\tau) \\nabla_{\\theta} \\log\\pi_{\\theta}(\\tau)r(\\tau)d\\tau\\\\\n",
    "= E_{\\tau \\sim \\pi\\theta(\\tau)}[\\nabla_\\theta \\log \\pi_\\theta(\\tau)r(\\tau)] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_learned_policy = utils.run_training_loop(env=rich_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(rich_env, augment_policies(policies, rich_learned_policy, \"Tech Haven Policy\"), \"COVID-19 Curves for Tech Haven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_reward_bars(rich_env, augment_policies(policies, rich_learned_policy, \"Tech Haven Policy\"), \"Rewards for Tech Haven Policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As anticipated, our agent learns that the best policy for Tech Haven is to perform almost no social distancing. It also has the highest reward among our static policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we look at Steel Town\n",
    "\n",
    "This place has the same human cost - that is the cost of an individual dying, however this location does not produce nearly as much economic output and so it can afford social distancing practices and we see that the model learns that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.poor_place_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_env = gym.make('COVID19-POOR-v0')\n",
    "poor_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_learned_policy = utils.run_training_loop(env=poor_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(poor_env, augment_policies(policies, poor_learned_policy, \"Steel Town Policy\"), \"COVID-19 Curves for Steel Town\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(poor_env, augment_policies(policies, poor_learned_policy, \"Steel Town Policy\"), \"Rewards for Steel Town Policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Steel Town, our learned policy balances curtailing the spread of COVID with the (low) economic cost of implementing social to implement a policy that is almost as strict as 100% social distancing, but realistic in allowing for some economic contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Locations\n",
    "\n",
    "Finally, we train our agent on an envrionment where confounder values (locations) vary probabilistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.confounded_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_env = gym.make('COVID19-CONFOUNDED-v0')\n",
    "confounded_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_learned_policy = utils.run_training_loop(env=confounded_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounder Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_policies = augment_policies({}, confounded_learned_policy, policy_name='Confounded Policy')\n",
    "plot_reward_bars(poor_env, augment_policies(augmented_policies, poor_learned_policy, policy_name='Steel Town Policy'),'Rewards: Confounded Policy vs Steel Town Policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(rich_env, augment_policies(augmented_policies, rich_learned_policy, policy_name='Tech Haven Policy'),'Rewards: Confounded Policy vs Tech Haven Policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see from these reward plots, a confounded policy that does not adjust for a causal confounder does disproprortionately worse than policies trained on fixed levels of the confounder for both Steel Town and Tech Haven. This suggests an interesting area of exploration in Reinforcement Learning - studying the causal structures of problems to help optimize decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://wwwnc.cdc.gov/eid/article/26/6/20-0233_article\n",
    "2. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6332839/"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
